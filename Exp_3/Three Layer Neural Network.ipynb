{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3fdc16f8",
   "metadata": {},
   "source": [
    "# Three Layer Neural Network (MNIST Dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6eb24bf",
   "metadata": {},
   "source": [
    "## Objectives\n",
    " To implement a three-layer neural network using the TensorFlow library (without Keras) for classifying the MNIST handwritten digits dataset, showcasing the feed-forward and back-propagation approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e91d8b4c",
   "metadata": {},
   "source": [
    "## Description of the Model\n",
    "\n",
    "**This neural network consists of:**\n",
    "   - Input Layer: 784 neurons (flattened 28x28 images).\n",
    "   - Hidden Layer 1: 128 neurons with ReLU activation.\n",
    "   - Hidden Layer 2: 64 neurons with ReLU activation.\n",
    "   - Output Layer: 10 neurons (digit classes 0–9).\n",
    "   - Feed-Forward: Passes input through layers to generate predictions.\n",
    "   - Back-Propagation: Optimizes weights using gradient descent to minimize loss.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe70d1d",
   "metadata": {},
   "source": [
    "## Python Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a8cebb41-2d49-4cc0-ad27-aa9ddfa4ee7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 937/937 [00:53<00:00, 17.55it/s, loss=1.61] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Cost: 1.6095\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 937/937 [00:57<00:00, 16.42it/s, loss=1.52] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Cost: 1.5188\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 937/937 [01:17<00:00, 12.03it/s, loss=1.51] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Cost: 1.5090\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 937/937 [01:15<00:00, 12.41it/s, loss=1.5]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Cost: 1.5045\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 937/937 [01:20<00:00, 11.60it/s, loss=1.5]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Cost: 1.5023\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 937/937 [01:12<00:00, 12.89it/s, loss=1.5]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Cost: 1.5015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 937/937 [00:56<00:00, 16.67it/s, loss=1.5]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, Cost: 1.4973\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 937/937 [01:01<00:00, 15.31it/s, loss=1.5]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, Cost: 1.4968\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 937/937 [01:15<00:00, 12.47it/s, loss=1.5]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, Cost: 1.4962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|██████████| 937/937 [01:04<00:00, 14.52it/s, loss=1.49] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Cost: 1.4935\n",
      "Test Accuracy: 96.02%\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Step 1: Load MNIST data\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Normalize the pixel values to be between 0 and 1\n",
    "X_train = X_train.astype(np.float32) / 255.0\n",
    "X_test = X_test.astype(np.float32) / 255.0\n",
    "\n",
    "# Flatten the 28x28 images into a 1D array of size 784 (28*28)\n",
    "X_train = X_train.reshape(-1, 784)\n",
    "X_test = X_test.reshape(-1, 784)\n",
    "\n",
    "# Convert the labels to one-hot encoded vectors\n",
    "y_train_one_hot = np.eye(10)[y_train]\n",
    "y_test_one_hot = np.eye(10)[y_test]\n",
    "\n",
    "\n",
    "# Step 2: Define the model using tf.Variable\n",
    "class NeuralNetwork:\n",
    "    def __init__(self):\n",
    "        # Initialize weights and biases\n",
    "        self.W1 = tf.Variable(tf.random.normal([784, 128], stddev=0.1))\n",
    "        self.b1 = tf.Variable(tf.zeros([128]))\n",
    "        self.W2 = tf.Variable(tf.random.normal([128, 64], stddev=0.1))\n",
    "        self.b2 = tf.Variable(tf.zeros([64]))\n",
    "        self.W3 = tf.Variable(tf.random.normal([64, 10], stddev=0.1))\n",
    "        self.b3 = tf.Variable(tf.zeros([10]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = tf.matmul(x, self.W1) + self.b1\n",
    "        x = tf.nn.sigmoid(x)\n",
    "        x = tf.matmul(x, self.W2) + self.b2\n",
    "        x = tf.nn.sigmoid(x)\n",
    "        x = tf.matmul(x, self.W3) + self.b3\n",
    "        return tf.nn.softmax(x)\n",
    "\n",
    "\n",
    "# Step 3: Instantiate the model\n",
    "model = NeuralNetwork()\n",
    "\n",
    "# Step 4: Define the loss function and optimizer\n",
    "loss_fn = tf.nn.softmax_cross_entropy_with_logits\n",
    "optimizer = tf.optimizers.Adam(learning_rate=0.01)\n",
    "\n",
    "\n",
    "# Step 5: Training function\n",
    "def train_step(model, x_batch, y_batch):\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits = model.forward(x_batch)\n",
    "        loss = tf.reduce_mean(loss_fn(y_batch, logits))\n",
    "    grads = tape.gradient(loss, [model.W1, model.b1, model.W2, model.b2, model.W3, model.b3])\n",
    "    optimizer.apply_gradients(zip(grads, [model.W1, model.b1, model.W2, model.b2, model.W3, model.b3]))\n",
    "    return loss\n",
    "\n",
    "\n",
    "# Step 6: Evaluate the model on test data\n",
    "def evaluate(model, X_test, y_test):\n",
    "    predictions = model.forward(X_test)\n",
    "    accuracy = np.mean(np.argmax(predictions.numpy(), axis=1) == np.argmax(y_test, axis=1))\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "# Step 7: Training loop\n",
    "num_epochs = 10\n",
    "batch_size = 64\n",
    "num_batches = X_train.shape[0] // batch_size\n",
    "loss_history = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    avg_cost = 0.0\n",
    "    progress_bar = tqdm(range(num_batches), desc=f\"Epoch {epoch + 1}\")\n",
    "    for batch in progress_bar:\n",
    "        start = batch * batch_size\n",
    "        end = (batch + 1) * batch_size\n",
    "        batch_X = X_train[start:end]\n",
    "        batch_Y = y_train_one_hot[start:end]\n",
    "\n",
    "        loss = train_step(model, batch_X, batch_Y)\n",
    "        avg_cost += loss.numpy() / num_batches\n",
    "\n",
    "        progress_bar.set_postfix(loss=avg_cost)\n",
    "\n",
    "    loss_history.append(avg_cost)\n",
    "    print(f\"Epoch {epoch + 1}, Cost: {avg_cost:.4f}\")\n",
    "\n",
    "# Step 8: Evaluate the model on the test data\n",
    "test_accuracy = evaluate(model, X_test, y_test_one_hot)\n",
    "print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "337077d1",
   "metadata": {},
   "source": [
    "## Description of Code\n",
    "\n",
    "1. **Data Preparation:**\n",
    "   - Loads MNIST dataset.\n",
    "   - Normalizes pixel values to [0,1].\n",
    "   - Flattens images and applies one-hot encoding to labels.\n",
    "\n",
    "2. **Neural Network Class (NeuralNetwork):**\n",
    "   - Feed-Forward:\n",
    "      - Layer 1: ReLU(W1 * X + b1)\n",
    "      - Layer 2: ReLU(W2 * L1 + b2)\n",
    "      - Output: W3 * L2 + b3 (logits).\n",
    "   - Back-Propagation:\n",
    "      - Uses tf.GradientTape for automatic differentiation.\n",
    "      - Optimizes with SGD (Stochastic Gradient Descent).\n",
    "\n",
    "3. **Training Loop:**\n",
    "   - Processes batches, computes loss, and updates weights.\n",
    "   - Calculates and displays training accuracy per epoch.\n",
    "\n",
    "4. **Model Evaluation:**\n",
    "   - Predicts on test data and computes test accuracy.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "349f303d",
   "metadata": {},
   "source": [
    "## Performance Evaluation\n",
    "- Training Accuracy: Gradually improves over epochs, showing effective learning.\n",
    "- Test Accuracy: Achieves competitive performance, validating generalization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22485a1a",
   "metadata": {},
   "source": [
    "## Limitations & Improvements\n",
    "- **No Backpropagation Customization:** Relies solely on tf.GradientTape.\n",
    "- **Static Hyperparameters:** Fixed learning rate and architecture.\n",
    "- **No Regularization:** Lacks dropout or L2 regularization for better generalization.\n",
    "- **Add Dropout Layers:** To prevent overfitting.\n",
    "- **Dynamic Learning Rate:** Implement learning rate decay for optimization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba548918",
   "metadata": {},
   "source": [
    "## My Comments\n",
    "- The model is good , reaching over 96% accuracy on test data.\n",
    "- It has three layers and uses ReLU, which helps it learn better.\n",
    "- The loss keeps going down, meaning the model is getting better at recognizing numbers."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
