{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3fdc16f8",
   "metadata": {},
   "source": [
    "# Multi-Layer Perceptron (MLP) Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6eb24bf",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "- Implement the Multi-Layer Perceptron (MLP) Learning Algorithm using NumPy in Python.\n",
    "- Evaluate the performance of a Multi-Layer Perceptron for **XOR** truth tables.\n",
    "- Use the **Step Function** as the activation function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e91d8b4c",
   "metadata": {},
   "source": [
    "## Description of the Model\n",
    "\n",
    "A **Multi-Layer Perceptron (MLP)** is a type of artificial neural network that consists of multiple layers of neurons. Unlike a single-layer perceptron, which can only solve linearly separable problems, an MLP can learn complex patterns, including the XOR Boolean function.\n",
    "\n",
    "#### Structure of MLP\n",
    "1. **Input Layer**  \n",
    "   - Receives input features (e.g., two binary inputs for XOR: [0,0], [0,1], etc.).\n",
    "   - Each neuron in this layer passes values to the next layer.\n",
    "\n",
    "2. **Hidden Layer**  \n",
    "   - This layer processes inputs using weights and biases.\n",
    "   - Each neuron in the hidden layer applies an **activation function** to determine its output.\n",
    "   - For this experiment, we use the **step function** instead of the traditional sigmoid.\n",
    "\n",
    "3. **Output Layer**  \n",
    "   - Produces the final predicted output.\n",
    "   - Uses the step function to return either 0 or 1.\n",
    "\n",
    "#### How MLP Learns\n",
    "1. **Forward Propagation:**  \n",
    "   - Inputs are passed through weighted connections, summed with biases, and processed through activation functions.  \n",
    "   - The network calculates the final output.\n",
    "\n",
    "2. **Error Calculation:**  \n",
    "   - The difference between the predicted and actual output is determined.  \n",
    "\n",
    "3. **Weight Updates (Backpropagation-like Process):**  \n",
    "   - Since we are not using a sigmoid function, a simplified update method is applied:\n",
    "     - The output error is propagated backward.  \n",
    "     - Weights and biases are adjusted using the **learning rate** to minimize the error.  \n",
    "\n",
    "#### Why MLP Works for XOR\n",
    "- The XOR function is **not linearly separable**, meaning a single-layer perceptron cannot classify it correctly.  \n",
    "- The **hidden layer** enables MLP to model more complex decision boundaries.  \n",
    "- By learning an intermediate representation, the network can correctly map XOR inputs to outputs.\n",
    "\n",
    "#### Performance Evaluation\n",
    "- We measure accuracy using `accuracy_score()`.  \n",
    "- A **confusion matrix** visualizes the classification performance.  \n",
    "- If the accuracy is 1.0, it means the MLP has learned XOR perfectly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe70d1d",
   "metadata": {},
   "source": [
    "## Implementation of MLP on XOR in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a8cebb41-2d49-4cc0-ad27-aa9ddfa4ee7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions for XOR: [0 1 1 0]\n",
      "Accuracy: 1.0\n",
      "Confusion Matrix:\n",
      " [[2 0]\n",
      " [0 2]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "def step_function(x):\n",
    "    return np.where(x >= 0, 1, 0)\n",
    "\n",
    "class Perceptron:\n",
    "    def __init__(self, input_size, learning_rate=0.1, epochs=10000):\n",
    "        self.input_size = input_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "        self.weights = np.random.randn(input_size)\n",
    "        self.bias = np.random.randn()\n",
    "\n",
    "    def forward(self, X):\n",
    "        return step_function(np.dot(X, self.weights) + self.bias)\n",
    "\n",
    "    def train(self, X, y):\n",
    "        for _ in range(self.epochs):\n",
    "            for xi, target in zip(X, y):\n",
    "                output = self.forward(xi)\n",
    "                error = target - output\n",
    "                self.weights += self.learning_rate * error * xi\n",
    "                self.bias += self.learning_rate * error\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.forward(X)\n",
    "\n",
    "# XOR dataset\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y_hidden1 = np.array([1, 1, 1, 0])\n",
    "y_hidden2 = np.array([0, 1, 1, 1])\n",
    "y_output = np.array([0, 1, 1, 0])\n",
    "\n",
    "# Train first hidden neuron\n",
    "hidden1 = Perceptron(input_size=2)\n",
    "hidden1.train(X, y_hidden1)\n",
    "\n",
    "# Train second hidden neuron\n",
    "hidden2 = Perceptron(input_size=2)\n",
    "hidden2.train(X, y_hidden2)\n",
    "\n",
    "# Combine hidden layer outputs\n",
    "hidden_output = np.column_stack((hidden1.predict(X), hidden2.predict(X)))\n",
    "\n",
    "# Train output neuron\n",
    "output_neuron = Perceptron(input_size=2)\n",
    "output_neuron.train(hidden_output, y_output)\n",
    "\n",
    "# Predictions\n",
    "predictions = output_neuron.predict(hidden_output)\n",
    "print(\"Predictions for XOR:\", predictions.flatten())\n",
    "\n",
    "# Flatten predictions\n",
    "y_pred = predictions.flatten()\n",
    "\n",
    "# Accuracy\n",
    "accuracy = accuracy_score(y_output, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Confusion matrix\n",
    "conf_matrix = confusion_matrix(y_output, y_pred)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "337077d1",
   "metadata": {},
   "source": [
    "## Description of Code\n",
    "\n",
    "#### Functions Defined:\n",
    "\n",
    "1. **Step Activation Function:**\n",
    "   - Implements a step function that returns 1 if `x >= 0`, otherwise 0.\n",
    "\n",
    "2. **Perceptron Class:**\n",
    "   - Initialization: Random weights and bias.\n",
    "   - Forward Pass: Computes output using weighted sum + step function.\n",
    "   - Training: Updates weights/bias using error correction.\n",
    "   - Prediction: Generates output for new data.\n",
    "\n",
    "3. **MLP architecture:**\n",
    "   - Hidden Layer: Two perceptrons trained with intermediate targets.\n",
    "   - Output Layer: Single perceptron trained on hidden layer outputs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "349f303d",
   "metadata": {},
   "source": [
    "## Performance Evaluation\n",
    "- **Confusion Matrix** is used to visualize classification performance.\n",
    "- **Accuracy** metric shows correct predictions.\n",
    "\n",
    "**Observations:**\n",
    "- The MLP successfully classifies XOR with 100% accuracy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22485a1a",
   "metadata": {},
   "source": [
    "## My Comments (Limitations & Improvements)\n",
    "- The perceptron is trying to solve the XOR problem, which a single-layer perceptron can't do. So, the code cleverly uses two hidden neurons to handle it.\n",
    "- The step function is used for activation.\n",
    "- The model learns through updates to its weights and bias using a simple perceptron learning rule.\n",
    "- It trains three perceptrons: two for the hidden layer and one for the output.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
